{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_w0phvrs-nkf"
   },
   "source": [
    "![Image of Venturenix](http://www.venturenix.com//assets/images/venture-nix-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gUVpCku-nki"
   },
   "source": [
    "# Ensemble\n",
    "AnthonyLo@VenturenixLab [2018]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dn1-BGdl-nkk"
   },
   "outputs": [],
   "source": [
    "from itertools import product, combinations, permutations\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRgbi1UY-nkp"
   },
   "source": [
    "## \"The wisdom of the masse exceed that of the wisest individual.\"\n",
    "\n",
    "**Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone**\n",
    "(https://en.wikipedia.org/wiki/Ensemble_learning#Bootstrap_aggregating_(bagging))\n",
    "- **Regression:** Take the average result from multiple regression models\n",
    "- **Classification:** Take the majority vote or average probability\n",
    "\n",
    "** Two assumptions:**\n",
    "\n",
    "- **Accurate:** The model has to be better than random guessing\n",
    "- **Independent:** The models are independent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pc5ek4hj-nkr"
   },
   "source": [
    "## voting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oh6XCgH0-nks"
   },
   "source": [
    "## Bagging (Bootstrap aggregating)\n",
    "\n",
    "- Improve stability and accuracy\n",
    "- Reduce variance and avoid fitting\n",
    "- A special case for model averaging\n",
    "\n",
    "![](File_Ozone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2UU6dB5-nku"
   },
   "source": [
    "## Boosting\n",
    "- emphasize the training instances that previous models mis-classified\n",
    "- yield better accuracy than bagging\n",
    "- tends to be more likely to over-fit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmBgJsXv-nkv"
   },
   "source": [
    "## Decision Tree (the building block)\n",
    "![](dt1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iooGNT8b-tV9"
   },
   "source": [
    "**What is Decision Tree?**\n",
    "\n",
    "\n",
    "\n",
    "*   each node represents a feature\n",
    "*   each branch represents a decision\n",
    "* each leaf represents an outcome (categorical or continues value)\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/660/1*XMId5sJqPtm8-RIwVVz2tg.png\" width=\"400\">\n",
    "\n",
    "\n",
    "**How we make a decision?/What is the learning algorithm?**\n",
    "\n",
    "* Select a feature that best classifies the training data and become the root of the tree. Repeat this process for each branch.\n",
    "\n",
    "\n",
    "* This means we are performing top-down, greedy search through the space of possible decision trees. (**Recursive Binary Splitting**). \n",
    "\n",
    "* Greedy Algorithm - Does not try variables that have been used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gq8ydEIE-3F-"
   },
   "source": [
    "**How do we select the best variable for splitting? **\n",
    "\n",
    "*  Select the feature that maximize the information gain\n",
    "\n",
    "> Equation: $IG(T,a) = H(T) - H(T|a)$\n",
    "\n",
    "> where $H(T|a)$ is the conditional entropy of T given the value of attributes $a$\n",
    "\n",
    "* **Information Gain = Entropy Before Split - Entropy after split**\n",
    "* You might also refer as: **Information Gain = Entropy(parent) - [weightes average] * Entropy(children)**\n",
    "\n",
    "**What is Entropy? **\n",
    "\n",
    "In binary Class:\n",
    "> $Entropy = \\sum_i - p_i \\log_2 p_i$\n",
    "* $p_i$ is the probability of class i\n",
    "* if all example belong to the same class = $entropy = - 1 \\log_2 1 = 0$  (Minimum Impurity)\n",
    "* if 50% in either class,  $entropy = - 0.5 \\log_2 0.5 - 0.5 \\log_2 0.5  = 1$ (Maximum impurity)\n",
    "* when we can classify better after the split, we can archive a lower entropy.\n",
    "\n",
    "https://homes.cs.washington.edu/~shapiro/EE596/notes/InfoGain.pdf\n",
    "\n",
    "\n",
    "Advantages of Decsion Tree:\n",
    "* Easy to interpret\n",
    "* No need to normalize data\n",
    "\n",
    "Disadvantage: \n",
    "\n",
    "*   Easier to overfit when a tree gets deeper.\n",
    "\n",
    "**Pruning**:  limiting tree depth to reduce overfitting in decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5dnRHZc-nkx"
   },
   "source": [
    "**Split by pureity**\n",
    "\n",
    "Entropy = - p(a)*log(p(a)) - p(b)*log(p(b))\n",
    "\n",
    "![](ent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ok8sJ6LC-nkx"
   },
   "source": [
    "## Random Forest (Bagging)\n",
    "![](rf1.png)\n",
    "\n",
    "- robust to correlated predictors\n",
    "- can solve both regression and classifcation problems\n",
    "- can handle thousands of input variables\n",
    "- used as a feature selection tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDYOZbhd-nky"
   },
   "source": [
    "Sklearn: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2H4nKdzJ-nkz"
   },
   "source": [
    "## XGBoost (Boosting)\n",
    "Installation:  \n",
    "for Mac: >conda install py-xgboost,\n",
    "<br>\n",
    "For Windows: https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost\n",
    "run > pip install xxx.whl\n",
    "\n",
    "XGboost: https://xgboost.readthedocs.io/en/latest/\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "![](xgb1.png)\n",
    "\n",
    "- A very common and powerful library for Kaggle compeition\n",
    "- Regularization supported\n",
    "- Parallel processing\n",
    "- Custom optimization objectives and evaluation criteria\n",
    "- Handle missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOTWBwSv-nk0"
   },
   "source": [
    "## Project: Digit Recognizer (MNIST)\n",
    "\n",
    "https://www.kaggle.com/c/digit-recognizer/\n",
    "![](1.png)\n",
    "![](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - py-xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0           8 KB\n",
      "    conda-4.8.4                |           py37_0         2.9 MB\n",
      "    libxgboost-0.90            |       h0a44026_1         1.2 MB\n",
      "    py-xgboost-0.90            |   py37h0a44026_1          75 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         4.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  pkgs/main/osx-64::_py-xgboost-mutex-2.0-cpu_0\n",
      "  libxgboost         pkgs/main/osx-64::libxgboost-0.90-h0a44026_1\n",
      "  py-xgboost         pkgs/main/osx-64::py-xgboost-0.90-py37h0a44026_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                                        4.8.2-py37_0 --> 4.8.4-py37_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install py-xgboost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 regularization\n",
    "https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "07_ensemble_randomforest_xgboost.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
